{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record video and audio simultaniously\n",
    "import pyaudio\n",
    "import wave\n",
    "import threading\n",
    "import time\n",
    "import subprocess\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wf\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record audio\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"siva.wav\"\n",
    "\n",
    "class recorder:\n",
    "    def __init__(self):\n",
    "        self.going = False\n",
    "        self.process = None\n",
    "        self.filename = \"ScreenCapture.mpg\"\n",
    "    def record(self,filename):\n",
    "        try:\n",
    "            if self.process.is_alive():\n",
    "                self.going = False\n",
    "        except AttributeError:\n",
    "            print(\"test\")\n",
    "        self.process = threading.Thread(target=self._record)\n",
    "        self.process.start()\n",
    "        self.filename = filename\n",
    "    def _record(self):\n",
    "        p = pyaudio.PyAudio()\n",
    "        stream = p.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "\n",
    "        print(\"* recording\")\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        self.going = True\n",
    "        \n",
    "        while self.going:\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "\n",
    "        print(\"* done recording\")\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "        wf.close()\n",
    "        \n",
    "\n",
    "    def stop_recording(self):\n",
    "        self.going = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "* recording\n"
     ]
    }
   ],
   "source": [
    "audio = recorder()\n",
    "audio.record(audio.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* done recording\n"
     ]
    }
   ],
   "source": [
    "audio.stop_recording()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceActivityDetector():\n",
    "    \n",
    "    def __init__(self, wave_input_filename):\n",
    "        self._read_wav(wave_input_filename)._convert_to_mono()\n",
    "        self.sample_window = 0.02 #20 ms\n",
    "        self.sample_overlap = 0.01 #10ms\n",
    "        self.speech_window = 0.5 #half a second\n",
    "        self.speech_energy_threshold = 0.6 #60% of energy in voice band\n",
    "        self.speech_start_band = 300\n",
    "        self.speech_end_band = 3000\n",
    "           \n",
    "    def _read_wav(self, wave_file):\n",
    "        self.rate, self.data = wf.read(wave_file)\n",
    "        self.channels = len(self.data.shape)\n",
    "        self.filename = wave_file\n",
    "        return self\n",
    "    \n",
    "    def _convert_to_mono(self):\n",
    "        if self.channels == 2 :\n",
    "            self.data = np.mean(self.data, axis=1, dtype=self.data.dtype)\n",
    "            self.channels = 1\n",
    "        return self\n",
    "    \n",
    "    def _calculate_frequencies(self, audio_data):\n",
    "        data_freq = np.fft.fftfreq(len(audio_data),1.0/self.rate)\n",
    "        data_freq = data_freq[1:]\n",
    "        return data_freq    \n",
    "    \n",
    "    def _calculate_amplitude(self, audio_data):\n",
    "        data_ampl = np.abs(np.fft.fft(audio_data))\n",
    "        data_ampl = data_ampl[1:]\n",
    "        return data_ampl\n",
    "        \n",
    "    def _calculate_energy(self, data):\n",
    "        data_amplitude = self._calculate_amplitude(data)\n",
    "        data_energy = data_amplitude ** 2\n",
    "        return data_energy\n",
    "        \n",
    "    def _znormalize_energy(self, data_energy):\n",
    "        energy_mean = np.mean(data_energy)\n",
    "        energy_std = np.std(data_energy)\n",
    "        energy_znorm = (data_energy - energy_mean) / energy_std\n",
    "        return energy_znorm\n",
    "    \n",
    "    def _connect_energy_with_frequencies(self, data_freq, data_energy):\n",
    "        energy_freq = {}\n",
    "        for (i, freq) in enumerate(data_freq):\n",
    "            if abs(freq) not in energy_freq:\n",
    "                energy_freq[abs(freq)] = data_energy[i] * 2\n",
    "        return energy_freq\n",
    "    \n",
    "    def _calculate_normalized_energy(self, data):\n",
    "        data_freq = self._calculate_frequencies(data)\n",
    "        data_energy = self._calculate_energy(data)\n",
    "        #data_energy = self._znormalize_energy(data_energy) #znorm brings worse results\n",
    "        energy_freq = self._connect_energy_with_frequencies(data_freq, data_energy)\n",
    "        return energy_freq\n",
    "    \n",
    "    def _sum_energy_in_band(self,energy_frequencies, start_band, end_band):\n",
    "        sum_energy = 0\n",
    "        for f in energy_frequencies.keys():\n",
    "            if start_band<f<end_band:\n",
    "                sum_energy += energy_frequencies[f]\n",
    "        return sum_energy\n",
    "    \n",
    "    def _median_filter (self, x, k):\n",
    "        assert k % 2 == 1, \"Median filter length must be odd.\"\n",
    "        assert x.ndim == 1, \"Input must be one-dimensional.\"\n",
    "        k2 = (k - 1) // 2\n",
    "        y = np.zeros ((len (x), k), dtype=x.dtype)\n",
    "        y[:,k2] = x\n",
    "        for i in range (k2):\n",
    "            j = k2 - i\n",
    "            y[j:,i] = x[:-j]\n",
    "            y[:j,i] = x[0]\n",
    "            y[:-j,-(i+1)] = x[j:]\n",
    "            y[-j:,-(i+1)] = x[-1]\n",
    "        return np.median (y, axis=1)\n",
    "        \n",
    "    def _smooth_speech_detection(self, detected_windows):\n",
    "        median_window=int(self.speech_window/self.sample_window)\n",
    "        if median_window%2==0: median_window=median_window-1\n",
    "        median_energy = self._median_filter(detected_windows[:,1], median_window)\n",
    "        return median_energy\n",
    "        \n",
    "    def convert_windows_to_readible_labels(self, detected_windows):\n",
    "        \"\"\" Takes as input array of window numbers and speech flags from speech\n",
    "        detection and convert speech flags to time intervals of speech.\n",
    "        Output is array of dictionaries with speech intervals.\n",
    "        \"\"\"\n",
    "        speech_time = []\n",
    "        is_speech = 0\n",
    "        for window in detected_windows:\n",
    "            if (window[1]==1.0 and is_speech==0): \n",
    "                is_speech = 1\n",
    "                speech_label = {}\n",
    "                speech_time_start = window[0] / self.rate\n",
    "                speech_label['speech_begin'] = speech_time_start\n",
    "                print(window[0], speech_time_start)\n",
    "                #speech_time.append(speech_label)\n",
    "            if (window[1]==0.0 and is_speech==1):\n",
    "                is_speech = 0\n",
    "                speech_time_end = window[0] / self.rate\n",
    "                speech_label['speech_end'] = speech_time_end\n",
    "                speech_time.append(speech_label)\n",
    "                print(window[0], speech_time_end)\n",
    "        return speech_time\n",
    "      \n",
    "    def plot_detected_speech_regions(self):\n",
    "        \"\"\" Performs speech detection and plot original signal and speech regions.\n",
    "        \"\"\"\n",
    "        data = self.data\n",
    "        detected_windows = self.detect_speech()\n",
    "        data_speech = np.zeros(len(data))\n",
    "        it = np.nditer(detected_windows[:,0], flags=['f_index'])\n",
    "        while not it.finished:\n",
    "            data_speech[int(it[0])] = data[int(it[0])] * detected_windows[it.index,1]\n",
    "            it.iternext()\n",
    "        plt.figure()\n",
    "        plt.plot(data_speech)\n",
    "        plt.plot(data)\n",
    "        plt.show()\n",
    "        return self\n",
    "       \n",
    "    def detect_speech(self):\n",
    "        \"\"\" Detects speech regions based on ratio between speech band energy\n",
    "        and total energy.\n",
    "        Output is array of window numbers and speech flags (1 - speech, 0 - nonspeech).\n",
    "        \"\"\"\n",
    "        detected_windows = np.array([])\n",
    "        sample_window = int(self.rate * self.sample_window)\n",
    "        sample_overlap = int(self.rate * self.sample_overlap)\n",
    "        data = self.data\n",
    "        sample_start = 0\n",
    "        start_band = self.speech_start_band\n",
    "        end_band = self.speech_end_band\n",
    "        while (sample_start < (len(data) - sample_window)):\n",
    "            sample_end = sample_start + sample_window\n",
    "            if sample_end>=len(data): sample_end = len(data)-1\n",
    "            data_window = data[sample_start:sample_end]\n",
    "            energy_freq = self._calculate_normalized_energy(data_window)\n",
    "            sum_voice_energy = self._sum_energy_in_band(energy_freq, start_band, end_band)\n",
    "            sum_full_energy = sum(energy_freq.values())\n",
    "            speech_ratio = sum_voice_energy/sum_full_energy\n",
    "            # Hipothesis is that when there is a speech sequence we have ratio of energies more than Threshold\n",
    "            speech_ratio = speech_ratio>self.speech_energy_threshold\n",
    "            detected_windows = np.append(detected_windows,[sample_start, speech_ratio])\n",
    "            sample_start += sample_overlap\n",
    "        detected_windows = detected_windows.reshape(int(len(detected_windows)/2),2)\n",
    "        detected_windows[:,1] = self._smooth_speech_detection(detected_windows)\n",
    "        return detected_windows\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(data, filename):\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360.0 0.17\n",
      "2160.0 0.27\n",
      "11280.0 1.41\n",
      "13120.0 1.64\n",
      "14800.0 1.85\n",
      "15360.0 1.92\n",
      "36320.0 4.54\n",
      "37120.0 4.64\n",
      "44400.0 5.55\n",
      "45200.0 5.65\n",
      "53440.0 6.68\n",
      "55760.0 6.97\n",
      "60320.0 7.54\n",
      "60400.0 7.55\n",
      "81440.0 10.18\n",
      "82960.0 10.37\n",
      "91040.0 11.38\n",
      "93600.0 11.7\n",
      "105280.0 13.16\n",
      "105760.0 13.22\n",
      "110160.0 13.77\n",
      "110240.0 13.78\n",
      "110320.0 13.79\n",
      "111920.0 13.99\n",
      "122560.0 15.32\n",
      "123040.0 15.38\n",
      "131760.0 16.47\n"
     ]
    }
   ],
   "source": [
    "v = VoiceActivityDetector(\"ENG_M.wav\")\n",
    "raw_detection = v.detect_speech()\n",
    "speech_labels = v.convert_windows_to_readible_labels(raw_detection)\n",
    "    \n",
    "save_to_file(speech_labels, \"siva.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
